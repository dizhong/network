#!/usr/bin/env python3

import csv
import requests
import sys
import os

# manage stored files
#   might read file on disk with most popular content, download 'till we have ~9MB
def cache_files(filename, origin):
    f = open(filename, 'r')
    line_reader = csv.reader(f)
    cache_size = 0
    # skip header
    next(line_reader)
    for line in line_reader:
        path = line[0]
        page_name = path[len("https://en.wikipedia.org"):]
        r = requests.get("http://" + origin + ":8080" + page_name)

        # cached_paths[page_name] = r.content
        output = open("cache" + page_name, "wb")
        output.write(r.content)
        output.close()

        cache_size += len(r.content)
        if cache_size >= 8000000:
            break

    print("cache loaded")
    f.close()


if __name__ == "__main__":
    ORIGIN = sys.argv[1]
    os.makedirs("cache/wiki", exist_ok=True)
    cache_files("popular_pages_20201130.csv", ORIGIN)
